# improved_pytorch_trend_prediction_model.py
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, r2_score
from scipy.signal import find_peaks
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®éšæœºç§å­
torch.manual_seed(42)
np.random.seed(42)

# æ£€æŸ¥è®¾å¤‡
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ä½¿ç”¨è®¾å¤‡: {device}")

# ---------------------- è¶‹åŠ¿æå–å‡½æ•° ----------------------
def extract_moving_average_trend(signal, window=101):
    """æå–æ»‘åŠ¨å¹³å‡è¶‹åŠ¿"""
    pad_len = window // 2
    padded = np.pad(signal, (pad_len, pad_len), mode='reflect')
    return np.convolve(padded, np.ones(window)/window, mode='valid')

def find_threshold_from_data(df, trend_real, window=501):
    """ä»æ•°æ®ä¸­æ‰¾åˆ°thresholdå€¼"""
    # è®¡ç®—è¶‹åŠ¿çš„ä¸€é˜¶ä¸äºŒé˜¶å¯¼æ•°
    first_derivative = np.gradient(trend_real, df['f1'])
    second_derivative = np.gradient(first_derivative, df['f1'])
    
    # æ‰¾åˆ°å¤šä¸ªäºŒé˜¶å¯¼æ•°å˜åŒ–è¾ƒå¤§çš„ç‚¹ï¼ˆæ‹ç‚¹ï¼‰
    peak_indices, _ = find_peaks(np.abs(second_derivative), distance=5000, prominence=1e-10)
    
    if len(peak_indices) > 0:
        peak_times = df['f1'].iloc[peak_indices].values
        peak_f9s = trend_real[peak_indices]
        threshold_value = peak_f9s[-1]  # æœ€åä¸€ä¸ªæ‹ç‚¹çš„f9å€¼
        threshold_time = peak_times[-1]  # æœ€åä¸€ä¸ªæ‹ç‚¹çš„æ—¶é—´
        return threshold_value, threshold_time, peak_indices
    else:
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ‹ç‚¹ï¼Œä½¿ç”¨è¶‹åŠ¿çš„æœ€å¤§å€¼
        max_idx = np.argmax(trend_real)
        return trend_real[max_idx], df['f1'].iloc[max_idx], [max_idx]

# ---------------------- æ•°æ®é›†ç±» ----------------------
class TrendDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.FloatTensor(X)
        self.y = torch.FloatTensor(y)
    
    def __len__(self):
        return len(self.X)
    
    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

# ---------------------- æ”¹è¿›çš„LSTMæ¨¡å‹å®šä¹‰ ----------------------
class ImprovedTrendLSTM(nn.Module):
    def __init__(self, input_size, hidden_size=64, output_size=2):
        super(ImprovedTrendLSTM, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)
        self.norm = nn.LayerNorm(hidden_size)
        self.fc1 = nn.Linear(hidden_size, 32)
        self.fc2 = nn.Linear(32, output_size)
        self.relu = nn.ReLU()

    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        last_output = lstm_out[:, -1, :]
        last_output = self.norm(last_output)
        out = self.relu(self.fc1(last_output))
        out = self.fc2(out)
        return out

# ---------------------- æ•°æ®é¢„å¤„ç†å‡½æ•° ----------------------
def create_sequences_with_trend_info(data, seq_length, include_derivatives=True):
    """åˆ›å»ºåŒ…å«è¶‹åŠ¿ä¿¡æ¯çš„LSTMè®­ç»ƒåºåˆ—"""
    X, y = [], []
    
    if include_derivatives and data.shape[1] >= 2:
        # è®¡ç®—è¶‹åŠ¿çš„å¯¼æ•°ä¿¡æ¯
        trend_values = data[:, 0]  # å‡è®¾ç¬¬ä¸€åˆ—æ˜¯è¶‹åŠ¿å€¼
        first_derivative = np.gradient(trend_values)
        second_derivative = np.gradient(first_derivative)
        
        # æ·»åŠ å¯¼æ•°ä¿¡æ¯ä½œä¸ºé¢å¤–ç‰¹å¾
        enhanced_data = np.column_stack([data, first_derivative, second_derivative])
    else:
        enhanced_data = data
    
    for i in range(len(enhanced_data) - seq_length):
        X.append(enhanced_data[i:(i + seq_length)])
        y.append(enhanced_data[i + seq_length, :2])  # åªé¢„æµ‹åŸå§‹çš„ä¸¤ä¸ªç‰¹å¾
    
    return np.array(X), np.array(y)

def prepare_data_for_training(df, seq_length=50, window=501):
    """å‡†å¤‡è®­ç»ƒæ•°æ® - å¢å¼ºç‰ˆ"""
    # æ•°æ®é¢„å¤„ç†
    df = df.sort_values(by='f1').reset_index(drop=True)
    
    # ä½¿ç”¨æ›´ç¨³å¥çš„æ ‡å‡†åŒ–æ–¹æ³•
    scaler = MinMaxScaler(feature_range=(-1, 1))  # ä½¿ç”¨[-1, 1]èŒƒå›´ä»¥ä¿æŒè´Ÿå€¼ä¿¡æ¯
    f9_scaled = scaler.fit_transform(df[['f9']].values).flatten()
    
    # æå–è¶‹åŠ¿
    trend = extract_moving_average_trend(f9_scaled, window=window)
    trend_real = scaler.inverse_transform(trend.reshape(-1, 1)).flatten()
    
    # æ‰¾åˆ°threshold
    threshold_value, threshold_time, peak_indices = find_threshold_from_data(df, trend_real, window)
    
    # åˆ›å»ºå¢å¼ºçš„æ—¶é—´ç‰¹å¾
    time_values = df['f1'].values
    time_normalized = (time_values - time_values.min()) / (time_values.max() - time_values.min())
    
    # æ·»åŠ æ›´å¤šæ—¶é—´ç‰¹å¾ä»¥å¸®åŠ©æ¨¡å‹ç†è§£è¶‹åŠ¿
    time_squared = time_normalized ** 2
    time_log = np.log1p(time_normalized)  # log(1+x) é¿å…log(0)
    
    # ç»„åˆç‰¹å¾ï¼šè¶‹åŠ¿å€¼ + å¤šç§æ—¶é—´ç‰¹å¾
    features = np.column_stack([trend_real, time_normalized, time_squared, time_log])
    
    # åˆ›å»ºåŒ…å«å¯¼æ•°ä¿¡æ¯çš„åºåˆ—æ•°æ®
    X, y = create_sequences_with_trend_info(features, seq_length, include_derivatives=True)
    
    return X, y, trend_real, threshold_value, threshold_time, scaler, features, time_values

# ---------------------- æ”¹è¿›çš„è®­ç»ƒå‡½æ•° ----------------------
def train_model_with_threshold_stopping(model, train_loader, val_loader, threshold_value, 
                                      num_epochs=50, learning_rate=0.001):
    """è®­ç»ƒæ¨¡å‹ï¼Œå½“é¢„æµ‹å€¼è¾¾åˆ°é˜ˆå€¼æ—¶æå‰åœæ­¢"""
    criterion = nn.MSELoss()
    
    # ä½¿ç”¨æ›´å¤æ‚çš„ä¼˜åŒ–å™¨è°ƒåº¦
    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)
    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)
    
    train_losses = []
    val_losses = []
    
    best_val_loss = float('inf')
    patience_count = 0
    early_stop_patience = 15
    # threshold_reached_count = 0
    
    for epoch in range(num_epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0.0
        # predictions_below_threshold = 0
        # total_predictions = 0
        
        for batch_X, batch_y in train_loader:
            batch_X, batch_y = batch_X.to(device), batch_y.to(device)
            
            optimizer.zero_grad()
            outputs = model(batch_X)
            
            # è®¡ç®—æŸå¤±
            loss = criterion(outputs, batch_y)
            
            # æ·»åŠ è¶‹åŠ¿çº¦æŸæŸå¤± - é¼“åŠ±ä¸‹é™è¶‹åŠ¿
            trend_pred = outputs[:, 0]  # è¶‹åŠ¿é¢„æµ‹å€¼
            trend_target = batch_y[:, 0]  # è¶‹åŠ¿ç›®æ ‡å€¼
            
            # å¦‚æœé¢„æµ‹å€¼å¤§äºç›®æ ‡å€¼ï¼Œå¢åŠ é¢å¤–çš„æƒ©ç½šï¼ˆé¼“åŠ±ä¸‹é™ï¼‰
            trend_penalty = torch.mean(torch.relu(trend_pred - trend_target) ** 2) * 0.5
            
            total_loss = loss + trend_penalty
            total_loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            train_loss += total_loss.item()
            
            # æ£€æŸ¥é¢„æµ‹å€¼æ˜¯å¦æ¥è¿‘é˜ˆå€¼
            # with torch.no_grad():
            #     pred_values = outputs[:, 0].cpu().numpy()
            #     below_threshold = np.sum(pred_values <= (threshold_value + 0.001))
            #     predictions_below_threshold += below_threshold
            #     total_predictions += len(pred_values)
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0.0
        val_predictions_below_threshold = 0
        val_total_predictions = 0
        
        with torch.no_grad():
            for batch_X, batch_y in val_loader:
                batch_X, batch_y = batch_X.to(device), batch_y.to(device)
                outputs = model(batch_X)
                loss = criterion(outputs, batch_y)
                val_loss += loss.item()
                
                # æ£€æŸ¥éªŒè¯é›†é¢„æµ‹å€¼
                # pred_values = outputs[:, 0].cpu().numpy()
                # below_threshold = np.sum(pred_values <= (threshold_value + 0.001))
                # val_predictions_below_threshold += below_threshold
                # val_total_predictions += len(pred_values)
        
        train_loss /= len(train_loader)
        val_loss /= len(val_loader)
        
        train_losses.append(train_loss)
        val_losses.append(val_loss)
        
        scheduler.step()
        
        # è®¡ç®—è¾¾åˆ°é˜ˆå€¼çš„æ¯”ä¾‹
        train_threshold_ratio = 0
        val_threshold_ratio = 0
        
        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}, '
              f'Train Threshold Ratio: {train_threshold_ratio:.3f}, Val Threshold Ratio: {val_threshold_ratio:.3f}')
        
        # å¦‚æœå¤§éƒ¨åˆ†é¢„æµ‹éƒ½è¾¾åˆ°äº†é˜ˆå€¼ï¼Œå¯ä»¥è€ƒè™‘æå‰åœæ­¢
        # if val_threshold_ratio > 0.8:  # 80%çš„é¢„æµ‹è¾¾åˆ°é˜ˆå€¼
        #     threshold_reached_count += 1
        #     if threshold_reached_count >= 3:  # è¿ç»­3ä¸ªepochéƒ½æœ‰é«˜æ¯”ä¾‹è¾¾åˆ°é˜ˆå€¼
        #         print(f'å¤§éƒ¨åˆ†é¢„æµ‹å·²è¾¾åˆ°é˜ˆå€¼ï¼Œåœ¨ç¬¬ {epoch+1} è½®æå‰åœæ­¢è®­ç»ƒ')
        #         break
        # else:
        #     threshold_reached_count = 0
        
        # Early stopping based on validation loss
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_count = 0
            torch.save(model.state_dict(), 'best_model.pth')
        else:
            patience_count += 1
        
        if patience_count >= early_stop_patience:
            print(f'éªŒè¯æŸå¤±æœªæ”¹å–„ï¼Œåœ¨ç¬¬ {epoch+1} è½®æå‰åœæ­¢è®­ç»ƒ')
            break
    
    # åŠ è½½æœ€ä½³æ¨¡å‹
    model.load_state_dict(torch.load('best_model.pth'))
    
    return train_losses, val_losses

# ---------------------- åŠ¨æ€é¢„æµ‹å‡½æ•° ----------------------
def predict_until_threshold(model, initial_sequence, threshold_value, time_start, time_step, 
                          tolerance=0.00001, max_steps=50000, verbose=True):
    """åŠ¨æ€é¢„æµ‹ç›´åˆ°è¾¾åˆ°é˜ˆå€¼æ‰åœæ­¢"""
    model.eval()
    
    current_sequence = torch.FloatTensor(initial_sequence).unsqueeze(0).to(device)
    current_time = time_start
    
    predicted_times = []
    predicted_values = []
    
    with torch.no_grad():
        for step in range(max_steps):
            # é¢„æµ‹ä¸‹ä¸€ä¸ªç‚¹
            prediction = model(current_sequence)
            pred_trend = prediction[0, 0].cpu().item()
            
            # æ›´æ–°æ—¶é—´
            current_time += time_step
            time_normalized = min(1.0, current_time)
            time_squared = time_normalized ** 2
            time_log = np.log1p(time_normalized)
            
            # è®°å½•é¢„æµ‹ç»“æœ
            predicted_times.append(current_time)
            predicted_values.append(pred_trend)
            
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°é˜ˆå€¼
            if pred_trend <= (threshold_value + tolerance):
                if verbose:
                    print(f"\nè¾¾åˆ°é˜ˆå€¼!")
                    print(f"æ­¥æ•°: {step + 1}")
                    print(f"é¢„æµ‹å€¼: {pred_trend:.6f}")
                    print(f"é˜ˆå€¼: {threshold_value:.6f}")
                    print(f"å½’ä¸€åŒ–æ—¶é—´: {current_time:.6f}")
                return current_time, np.array(predicted_values), step + 1, True
            
            # æ„é€ æ–°çš„ç‰¹å¾ç‚¹
            new_features = np.array([pred_trend, time_normalized, time_squared, time_log])
            
            # è®¡ç®—å¯¼æ•°ä¿¡æ¯ï¼ˆåŸºäºåºåˆ—çš„æœ€åå‡ ä¸ªç‚¹ï¼‰
            if len(predicted_values) >= 2:
                recent_trends = [current_sequence[0, -1, 0].cpu().item()] + [pred_trend]
                first_deriv = recent_trends[-1] - recent_trends[-2]
                second_deriv = 0  # ç®€åŒ–å¤„ç†
            else:
                first_deriv = 0
                second_deriv = 0
            
            # æ·»åŠ å¯¼æ•°ä¿¡æ¯
            enhanced_features = np.concatenate([new_features, [first_deriv, second_deriv]])
            new_point = torch.FloatTensor([enhanced_features]).to(device)
            
            # æ›´æ–°åºåˆ—
            current_sequence = torch.cat([current_sequence[:, 1:, :], new_point.unsqueeze(1)], dim=1)
            
            # æ˜¾ç¤ºè¿›åº¦
            if verbose and (step + 1) % 1000 == 0:
                print(f"æ­¥æ•°: {step + 1:5d}, å½“å‰å€¼: {pred_trend:.6f}, ç›®æ ‡: {threshold_value:.6f}, "
                      f"å·®è·: {threshold_value - pred_trend:.6f}")
    
    if verbose:
        print(f"\nåœ¨ {max_steps} æ­¥å†…æœªè¾¾åˆ°é˜ˆå€¼")
        print(f"æœ€ç»ˆé¢„æµ‹å€¼: {predicted_values[-1]:.6f}")
        print(f"ç›®æ ‡é˜ˆå€¼: {threshold_value:.6f}")
    
    return None, np.array(predicted_values), max_steps, False

# ---------------------- ä¸»è¦è®­ç»ƒå’Œé¢„æµ‹å‡½æ•° ----------------------
def train_and_predict_model(full_csv_file, partial_csv_file, seq_length=50, window=501, 
                          batch_size=32, epochs=100, fine_tune_epochs=50):
    """è®­ç»ƒæ¨¡å‹å¹¶è¿›è¡Œé¢„æµ‹ - æ”¹è¿›ç‰ˆ"""
    
    print("=" * 80)
    print("Step 1: åŠ è½½å®Œæ•´æ•°æ®é›†å¹¶è®­ç»ƒåˆå§‹æ¨¡å‹")
    print("=" * 80)
    
    # åŠ è½½å®Œæ•´æ•°æ®é›†
    df_full = pd.read_csv(full_csv_file)
    X_full, y_full, trend_full, threshold_value, threshold_time, scaler_full, features_full, times_full = prepare_data_for_training(
        df_full, seq_length, window
    )
    
    print(f"å®Œæ•´æ•°æ®é›†å¤§å°: {len(df_full)}")
    print(f"ç‰¹å¾åºåˆ—æ•°é‡: {X_full.shape[0]}")
    print(f"è¾“å…¥ç‰¹å¾ç»´åº¦: {X_full.shape[2]}")
    print(f"é˜ˆå€¼: {threshold_value:.6f}")
    print(f"é˜ˆå€¼æ—¶é—´: {threshold_time:.2e}")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    dataset_full = TrendDataset(X_full, y_full)
    train_size = int(0.8 * len(dataset_full))
    val_size = len(dataset_full) - train_size
    train_dataset, val_dataset = torch.utils.data.random_split(dataset_full, [train_size, val_size])
    
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
    
    # åˆ›å»ºæ”¹è¿›çš„æ¨¡å‹
    input_size = X_full.shape[2]
    model = ImprovedTrendLSTM(input_size=input_size, hidden_size=128, output_size=2).to(device)
    
    print(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    
    # è®­ç»ƒæ¨¡å‹
    print("\nå¼€å§‹åˆå§‹è®­ç»ƒ...")
    train_losses, val_losses = train_model_with_threshold_stopping(
        model, train_loader, val_loader, threshold_value, epochs, 0.001
    )
    
    print("\n" + "=" * 80)
    print("Step 2: åŠ è½½éƒ¨åˆ†æ•°æ®é›†å¹¶è¿›è¡Œå¾®è°ƒ")
    print("=" * 80)
    
    # åŠ è½½éƒ¨åˆ†æ•°æ®é›†ï¼ˆæ•´ä½“è¾“å…¥ï¼Œä¸å†åªå–å‰åŠéƒ¨åˆ†ï¼‰
    df_partial = pd.read_csv(partial_csv_file)
    
    print(f"éƒ¨åˆ†æ•°æ®é›†å¤§å°: {len(df_partial)}")
    
    # å‡†å¤‡å®Œæ•´çš„éƒ¨åˆ†æ•°æ®
    X_partial, y_partial, trend_partial, _, _, scaler_partial, features_partial, times_partial = prepare_data_for_training(
        df_partial, seq_length, window
    )
    
    print(f"éƒ¨åˆ†æ•°æ®ç‰¹å¾åºåˆ—æ•°é‡: {X_partial.shape[0]}")
    
    # åˆ›å»ºéƒ¨åˆ†æ•°æ®åŠ è½½å™¨
    dataset_partial = TrendDataset(X_partial, y_partial)
    train_size_partial = int(0.8 * len(dataset_partial))
    val_size_partial = len(dataset_partial) - train_size_partial
    train_dataset_partial, val_dataset_partial = torch.utils.data.random_split(
        dataset_partial, [train_size_partial, val_size_partial]
    )
    
    train_loader_partial = DataLoader(train_dataset_partial, batch_size=batch_size//2, shuffle=True)
    val_loader_partial = DataLoader(val_dataset_partial, batch_size=batch_size//2, shuffle=False)
    
    # å¾®è°ƒè®­ç»ƒ
    print("\nå¼€å§‹å¾®è°ƒè®­ç»ƒ...")
    fine_tune_losses, fine_tune_val_losses = train_model_with_threshold_stopping(
        model, train_loader_partial, val_loader_partial, threshold_value, fine_tune_epochs, 0.0001
    )
    
    print("\n" + "=" * 80)
    print("Step 3: åŠ¨æ€é¢„æµ‹ç›´åˆ°è¾¾åˆ°é˜ˆå€¼")
    print("=" * 80)
    
    # å‡†å¤‡é¢„æµ‹çš„åˆå§‹æ•°æ®
    initial_sequence = features_partial[-seq_length:]
    
    # è®¡ç®—å¯¼æ•°ä¿¡æ¯å¹¶æ·»åŠ åˆ°åˆå§‹åºåˆ—
    trend_values = initial_sequence[:, 0]
    first_derivative = np.gradient(trend_values)
    second_derivative = np.gradient(first_derivative)
    
    # æ‰©å±•åˆå§‹åºåˆ—ä»¥åŒ…å«å¯¼æ•°ä¿¡æ¯
    enhanced_initial_sequence = np.zeros((seq_length, features_partial.shape[1] + 2))
    enhanced_initial_sequence[:, :features_partial.shape[1]] = initial_sequence
    enhanced_initial_sequence[:, -2] = first_derivative
    enhanced_initial_sequence[:, -1] = second_derivative
    
    # è®¡ç®—æ—¶é—´æ­¥é•¿
    time_values = times_partial
    time_step = np.mean(np.diff(time_values)) / (time_values.max() - time_values.min())
    time_start = (time_values[-1] - time_values.min()) / (time_values.max() - time_values.min())
    
    print(f"é¢„æµ‹èµ·å§‹æ—¶é—´: {time_values[-1]:.2e}")
    print(f"æ—¶é—´æ­¥é•¿: {time_step:.8f}")
    print(f"ç›®æ ‡é˜ˆå€¼: {threshold_value:.6f}")
    print(f"å½“å‰è¶‹åŠ¿å€¼: {trend_partial[-1]:.6f}")
    print(f"è·ç¦»é˜ˆå€¼: {threshold_value - trend_partial[-1]:.6f}")
    
    # è¿›è¡ŒåŠ¨æ€é¢„æµ‹
    print("\nå¼€å§‹é¢„æµ‹...")
    predicted_time_norm, predicted_values, steps, reached_threshold = predict_until_threshold(
        model, enhanced_initial_sequence, threshold_value, time_start, time_step, 
        tolerance=0.001, max_steps=50000, verbose=True
    )
    
    # ç»“æœå¤„ç†å’Œå¯è§†åŒ–
    results = {
        'model': model,
        'threshold_value': threshold_value,
        'threshold_time': threshold_time,
        'reached_threshold': reached_threshold,
        'prediction_steps': steps,
        'predicted_values': predicted_values,
        'scaler_full': scaler_full,
        'scaler_partial': scaler_partial,
        'train_losses': train_losses,
        'val_losses': val_losses,
        'fine_tune_losses': fine_tune_losses,
        'fine_tune_val_losses': fine_tune_val_losses
    }
    
    if reached_threshold:
        predicted_time_real = predicted_time_norm * (time_values.max() - time_values.min()) + time_values.min()
        results['predicted_time'] = predicted_time_real
        
        print(f"\né¢„æµ‹æˆåŠŸ!")
        print(f"é¢„æµ‹è¾¾åˆ°é˜ˆå€¼çš„æ—¶é—´: {predicted_time_real:.2e}")
        print(f"é¢„æµ‹æ­¥æ•°: {steps:,}")
        print(f"ä»å½“å‰æ—¶é—´åˆ°è¾¾é˜ˆå€¼éœ€è¦: {predicted_time_real - time_values[-1]:.2e} æ—¶é—´å•ä½")
        print(f"æœ€ç»ˆé¢„æµ‹å€¼: {predicted_values[-1]:.6f}")
    else:
        results['predicted_time'] = None
        print(f"\nåœ¨ {steps:,} æ­¥å†…æœªèƒ½è¾¾åˆ°é˜ˆå€¼")
        print(f"æœ€ç»ˆé¢„æµ‹å€¼: {predicted_values[-1]:.6f}")
        print(f"ç›®æ ‡é˜ˆå€¼: {threshold_value:.6f}")
    
    # å¯è§†åŒ–ç»“æœ
    print("\n" + "=" * 80)
    print("Step 4: å¯è§†åŒ–ç»“æœ")
    print("=" * 80)
    
    visualize_results(df_full, df_partial, trend_full, trend_partial, 
                     predicted_values, results, times_partial)
    
    return results

def visualize_results(df_full, df_partial, trend_full, trend_partial, 
                     predicted_values, results, times_partial):
    """å¯è§†åŒ–æ‰€æœ‰ç»“æœ - ç®€åŒ–ç‰ˆï¼ˆæ— è¡¨æƒ…ç¬¦å·ï¼‰"""
    
    fig, axes = plt.subplots(2, 3, figsize=(18, 10))
    
    # 1. è®­ç»ƒæŸå¤±å†å²
    axes[0, 0].plot(results['train_losses'], label='Initial Training Loss', alpha=0.8)
    axes[0, 0].plot(results['val_losses'], label='Initial Validation Loss', alpha=0.8)
    
    start_idx = len(results['train_losses'])
    fine_tune_x = range(start_idx, start_idx + len(results['fine_tune_losses']))
    axes[0, 0].plot(fine_tune_x, results['fine_tune_losses'], 
                   label='Fine-tuning Loss', alpha=0.8, color='red')
    axes[0, 0].plot(fine_tune_x, results['fine_tune_val_losses'], 
                   label='Fine-tuning Val Loss', alpha=0.8, color='orange')
    
    axes[0, 0].set_title('Model Training History', fontsize=12, fontweight='bold')
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].set_ylabel('Loss')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)
    axes[0, 0].set_yscale('log')
    
    # 2. å®Œæ•´æ•°æ®é›†è¶‹åŠ¿
    axes[0, 1].plot(df_full['f1'], df_full['f9'], alpha=0.4, label='Original f9', color='lightblue')
    axes[0, 1].plot(df_full['f1'], trend_full, 'b-', label='Extracted Trend', linewidth=2)
    axes[0, 1].axhline(y=results['threshold_value'], color='red', linestyle='--', linewidth=2,
                      label=f'Threshold = {results["threshold_value"]:.4f}')
    axes[0, 1].axvline(x=results['threshold_time'], color='purple', linestyle='--', linewidth=2,
                      label=f'Threshold Time = {results["threshold_time"]:.2e}')
    axes[0, 1].set_title('Full Dataset Trend Analysis', fontsize=12, fontweight='bold')
    axes[0, 1].set_xlabel('Time (f1)')
    axes[0, 1].set_ylabel('f9')
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)
    
    # 3. éƒ¨åˆ†æ•°æ®é›†å’Œé¢„æµ‹
    axes[0, 2].plot(df_partial['f1'], df_partial['f9'], alpha=0.4, 
                   label='Original f9', color='lightgreen')
    axes[0, 2].plot(df_partial['f1'], trend_partial, 'g-', 
                   label='Extracted Trend', linewidth=2)
    axes[0, 2].axhline(y=results['threshold_value'], color='red', linestyle='--', linewidth=2,
                      label=f'Target Threshold = {results["threshold_value"]:.4f}')
    
    # é¢„æµ‹è½¨è¿¹
    if results['reached_threshold'] and results['predicted_time'] is not None:
        time_values = times_partial
        time_step_real = np.mean(np.diff(time_values))
        start_time = time_values[-1]
        pred_times = [start_time + i * time_step_real for i in range(1, len(predicted_values) + 1)]
        
        axes[0, 2].plot(pred_times, predicted_values, 'r-', linewidth=2, 
                       label='Prediction Trajectory', alpha=0.8)
        axes[0, 2].scatter([pred_times[-1]], [predicted_values[-1]], color='red', s=50,
                         label=f'Predicted Time: {results["predicted_time"]:.2e}')
    
    axes[0, 2].set_title('Partial Dataset & Prediction', fontsize=12, fontweight='bold')
    axes[0, 2].set_xlabel('Time (f1)')
    axes[0, 2].set_ylabel('f9')
    axes[0, 2].legend()
    axes[0, 2].grid(True, alpha=0.3)
    
    # 4. é¢„æµ‹è½¨è¿¹ä¸å®Œæ•´è¶‹åŠ¿å¯¹æ¯”
    if results['reached_threshold']:
        # è·å–å®Œæ•´æ•°æ®é›†åœ¨é¢„æµ‹æ—¶é—´èŒƒå›´å†…çš„éƒ¨åˆ†
        mask_full = (df_full['f1'] >= times_partial[0]) & (df_full['f1'] <= results['predicted_time'])
        axes[1, 0].plot(df_full['f1'][mask_full], trend_full[mask_full], 
                       'b-', label='Full Dataset Trend')
        
        # æ·»åŠ é¢„æµ‹è½¨è¿¹
        axes[1, 0].plot(pred_times, predicted_values, 'r-', 
                       label='Prediction Trajectory')
        
        # æ ‡è®°å…³é”®ç‚¹
        axes[1, 0].axvline(x=times_partial[-1], color='gray', linestyle=':', 
                          label='Prediction Start')
        axes[1, 0].axvline(x=results['predicted_time'], color='red', linestyle='--',
                          label='Predicted End')
        
        axes[1, 0].set_title('Prediction vs Full Trend', fontsize=12, fontweight='bold')
        axes[1, 0].set_xlabel('Time (f1)')
        axes[1, 0].set_ylabel('f9')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
    
    # 5. é¢„æµ‹è½¨è¿¹ä¸é˜ˆå€¼çš„è·ç¦»
    if len(predicted_values) > 0:
        gap_to_threshold = results['threshold_value'] - predicted_values
        
        axes[1, 1].plot(range(len(predicted_values)), gap_to_threshold, 
                       'm-', label='Gap to Threshold')
        
        # æ ‡è®°è¾¾åˆ°é˜ˆå€¼çš„ç‚¹
        if results['reached_threshold']:
            axes[1, 1].axvline(x=len(predicted_values)-1, color='red', linestyle='--',
                              label='Threshold Reached')
        
        axes[1, 1].set_title('Gap to Threshold During Prediction', fontsize=12, fontweight='bold')
        axes[1, 1].set_xlabel('Prediction Step')
        axes[1, 1].set_ylabel('Threshold - Predicted Value')
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)
        axes[1, 1].set_yscale('log')
    
    # 6. é¢„æµ‹å€¼å˜åŒ–è¶‹åŠ¿
    if len(predicted_values) > 0:
        axes[1, 2].plot(range(len(predicted_values)), predicted_values, 
                       'c-', label='Predicted Values')
        
        # æ·»åŠ é˜ˆå€¼çº¿
        axes[1, 2].axhline(y=results['threshold_value'], color='red', linestyle='--',
                          label='Threshold')
        
        axes[1, 2].set_title('Predicted Values Over Steps', fontsize=12, fontweight='bold')
        axes[1, 2].set_xlabel('Prediction Step')
        axes[1, 2].set_ylabel('Predicted Value')
        axes[1, 2].legend()
        axes[1, 2].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('trend_prediction_results.png')
    plt.show()

if __name__ == '__main__':
    # ä½¿ç”¨ç¤ºä¾‹
    full_csv_file = "tube1_mode_1-0-0.875.csv"  # å®Œæ•´æ•°æ®é›†
    partial_csv_file = "tube2_mode_1-0-0.875.csv"  # ç›¸ä¼¼çš„éƒ¨åˆ†æ•°æ®é›†
    
    # å¦‚æœæ²¡æœ‰ç¬¬äºŒä¸ªæ•°æ®é›†ï¼Œå¯ä»¥ä½¿ç”¨åŒä¸€ä¸ªæ•°æ®é›†è¿›è¡Œæµ‹è¯•
    # partial_csv_file = full_csv_file
    
    try:
        results = train_and_predict_model(
            full_csv_file=full_csv_file,
            partial_csv_file=partial_csv_file,
            seq_length=300,      # LSTMåºåˆ—é•¿åº¦
            window=2001,         # æ»‘åŠ¨å¹³å‡çª—å£
            batch_size=32,      # æ‰¹å¤§å°
            epochs=20,         # åˆå§‹è®­ç»ƒè½®æ•°
            fine_tune_epochs=10 # å¾®è°ƒè®­ç»ƒè½®æ•°
        )
        
        print("\n" + "=" * 80)
        print("ğŸ‰ æœ€ç»ˆç»“æœæ€»ç»“")
        print("=" * 80)
        print(f"âœ… æ¨¡å‹æˆåŠŸè®­ç»ƒå¹¶å¾®è°ƒ")
        print(f"ğŸ¯ é˜ˆå€¼: {results['threshold_value']:.6f}")
        
        if results['reached_threshold'] and results['predicted_time'] is not None:
            print(f"â° é¢„æµ‹è¾¾åˆ°é˜ˆå€¼çš„æ—¶é—´: {results['predicted_time']:.2e}")
            print(f"ğŸ“Š é¢„æµ‹æ‰€éœ€æ­¥æ•°: {results['prediction_steps']:,}")
            print(f"ğŸš€ é¢„æµ‹æˆåŠŸç‡: 100%")
            
            # è®¡ç®—é¢„æµ‹æ•ˆç‡
            time_diff = results['predicted_time'] - results.get('current_time', 0)
            if time_diff > 0:
                print(f"â³ é¢„æµ‹æ—¶é—´è·¨åº¦: {time_diff:.2e} æ—¶é—´å•ä½")
        else:
            print(f"âŒ æ¨¡å‹æœªèƒ½åœ¨ {results['prediction_steps']:,} æ­¥å†…é¢„æµ‹åˆ°è¾¾é˜ˆå€¼")
            print(f"ğŸ“ˆ æœ€ç»ˆé¢„æµ‹å€¼: {results['predicted_values'][-1]:.6f}")
            print(f"ğŸ“ è·ç¦»é˜ˆå€¼è¿˜æœ‰: {results['threshold_value'] - results['predicted_values'][-1]:.6f}")
            
        print(f"ğŸ§  æ¨¡å‹å¤æ‚åº¦: {sum(p.numel() for p in results['model'].parameters()):,} å‚æ•°")
        print(f"ğŸ’¾ ä½¿ç”¨è®¾å¤‡: {device}")
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        torch.save({
            'model_state_dict': results['model'].state_dict(),
            'threshold_value': results['threshold_value'],
            'scaler_full': results['scaler_full'],
            'scaler_partial': results['scaler_partial'],
            'seq_length': 50,
            'input_size': 2
        }, 'final_trend_prediction_model.pth')
        print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜è‡³: final_trend_prediction_model.pth")
            
    except FileNotFoundError as e:
        print(f"âŒ æ–‡ä»¶æœªæ‰¾åˆ°: {e}")
        print("è¯·ç¡®ä¿CSVæ–‡ä»¶å­˜åœ¨äºå½“å‰ç›®å½•ä¸­")
    except Exception as e:
        print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")
        print("è¯·æ£€æŸ¥æ•°æ®æ ¼å¼å’Œå‚æ•°è®¾ç½®")
        import traceback
        traceback.print_exc()

# ---------------------- æ¨¡å‹åŠ è½½å’Œä½¿ç”¨å‡½æ•° ----------------------
def load_and_use_saved_model(model_path, new_data_csv, seq_length=50):
    """
    åŠ è½½ä¿å­˜çš„æ¨¡å‹å¹¶ç”¨äºæ–°æ•°æ®é¢„æµ‹
    
    Args:
        model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
        new_data_csv: æ–°æ•°æ®CSVæ–‡ä»¶
        seq_length: åºåˆ—é•¿åº¦
    
    Returns:
        é¢„æµ‹ç»“æœ
    """
    try:
        # åŠ è½½æ¨¡å‹
        checkpoint = torch.load(model_path, map_location=device)
        
        # é‡å»ºæ¨¡å‹
        model = ImprovedTrendLSTM(input_size=checkpoint['input_size'], 
                         hidden_size=64, num_layers=2, output_size=2).to(device)
        model.load_state_dict(checkpoint['model_state_dict'])
        model.eval()
        
        # åŠ è½½æ•°æ®
        df = pd.read_csv(new_data_csv)
        
        # å–å‰åŠéƒ¨åˆ†æ•°æ®
        mid_point = len(df) // 2
        df_half = df.iloc[:mid_point].copy()
        
        # å‡†å¤‡æ•°æ®
        X, y, trend, _, _, scaler, features, times = prepare_data_for_training(
            df_half, seq_length, 501
        )
        
        # è·å–é˜ˆå€¼
        threshold_value = checkpoint['threshold_value']
        
        # å‡†å¤‡é¢„æµ‹åˆå§‹åºåˆ—
        initial_sequence = features[-seq_length:]
        
        # è®¡ç®—æ—¶é—´å‚æ•°
        time_step = np.mean(np.diff(times)) / (times.max() - times.min())
        time_start = (times[-1] - times.min()) / (times.max() - times.min())
        
        print(f"ğŸ”® ä½¿ç”¨ä¿å­˜çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹...")
        print(f"ğŸ¯ ç›®æ ‡é˜ˆå€¼: {threshold_value:.6f}")
        print(f"ğŸ“ å½“å‰å€¼: {trend[-1]:.6f}")
        
        # è¿›è¡Œé¢„æµ‹
        predicted_time_norm, predicted_values, steps, reached_threshold = predict_until_threshold(
            model, initial_sequence, threshold_value, time_start, time_step,
            tolerance=0.001, max_steps=50000, verbose=True
        )
        
        if reached_threshold:
            predicted_time_real = predicted_time_norm * (times.max() - times.min()) + times.min()
            print(f"âœ… é¢„æµ‹æˆåŠŸ! è¾¾åˆ°é˜ˆå€¼æ—¶é—´: {predicted_time_real:.2e}")
            return {
                'success': True,
                'predicted_time': predicted_time_real,
                'steps': steps,
                'final_value': predicted_values[-1]
            }
        else:
            print(f"âŒ æœªèƒ½è¾¾åˆ°é˜ˆå€¼")
            return {
                'success': False,
                'steps': steps,
                'final_value': predicted_values[-1] if len(predicted_values) > 0 else None
            }
            
    except Exception as e:
        print(f"âŒ åŠ è½½æ¨¡å‹æ—¶å‡ºé”™: {e}")
        return {'success': False, 'error': str(e)}

# ---------------------- æ‰¹é‡é¢„æµ‹å‡½æ•° ----------------------
def batch_predict_multiple_datasets(model_path, csv_files, seq_length=50):
    """
    æ‰¹é‡é¢„æµ‹å¤šä¸ªæ•°æ®é›†
    
    Args:
        model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
        csv_files: CSVæ–‡ä»¶åˆ—è¡¨
        seq_length: åºåˆ—é•¿åº¦
    
    Returns:
        æ‰€æœ‰é¢„æµ‹ç»“æœ
    """
    results = {}
    
    print(f"ğŸš€ å¼€å§‹æ‰¹é‡é¢„æµ‹ {len(csv_files)} ä¸ªæ•°æ®é›†...")
    
    for i, csv_file in enumerate(csv_files, 1):
        print(f"\n{'='*50}")
        print(f"ğŸ“Š å¤„ç†æ•°æ®é›† {i}/{len(csv_files)}: {csv_file}")
        print(f"{'='*50}")
        
        result = load_and_use_saved_model(model_path, csv_file, seq_length)
        results[csv_file] = result
        
        if result['success']:
            print(f"âœ… {csv_file}: é¢„æµ‹æˆåŠŸ")
        else:
            print(f"âŒ {csv_file}: é¢„æµ‹å¤±è´¥")
    
    # æ±‡æ€»ç»“æœ
    print(f"\n{'='*60}")
    print("ğŸ“ˆ æ‰¹é‡é¢„æµ‹ç»“æœæ±‡æ€»")
    print(f"{'='*60}")
    
    successful = sum(1 for r in results.values() if r['success'])
    total = len(results)
    
    print(f"ğŸ“Š æ€»æ•°æ®é›†: {total}")
    print(f"âœ… æˆåŠŸé¢„æµ‹: {successful}")
    print(f"âŒ å¤±è´¥é¢„æµ‹: {total - successful}")
    print(f"ğŸ¯ æˆåŠŸç‡: {successful/total*100:.1f}%")
    
    return results

# ---------------------- ä½¿ç”¨ç¤ºä¾‹ ----------------------
"""
ä½¿ç”¨ç¤ºä¾‹:

1. è®­ç»ƒæ–°æ¨¡å‹:
results = train_and_predict_model(
    full_csv_file="tube1_mode_1-0-0.875.csv",
    partial_csv_file="tube2_mode_1-0-0.875.csv"
)

2. ä½¿ç”¨ä¿å­˜çš„æ¨¡å‹:
result = load_and_use_saved_model(
    "final_trend_prediction_model.pth",
    "new_data.csv"
)

3. æ‰¹é‡é¢„æµ‹:
csv_files = ["data1.csv", "data2.csv", "data3.csv"]
results = batch_predict_multiple_datasets(
    "final_trend_prediction_model.pth",
    csv_files
)
"""

results = train_and_predict_model(
    full_csv_file="tube1_mode_1-0-0.875.csv",
    partial_csv_file="tube2_mode_1-0-0.875.csv"
)

